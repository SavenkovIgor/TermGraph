{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413b2b8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend(first_message)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mTermChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChatEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 76\u001b[0m, in \u001b[0;36mTermChat.get_answer\u001b[0;34m(self, user_message)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend(Message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_message))\n\u001b[1;32m     75\u001b[0m chat_messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mrole, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent} \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages]\n\u001b[0;32m---> 76\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     78\u001b[0m     messages\u001b[38;5;241m=\u001b[39mchat_messages,\n\u001b[1;32m     79\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mtemp,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer})\n",
      "File \u001b[0;32m~/Programming/TermGraph/.venv/lib/python3.12/site-packages/openai/_utils/_proxy.py:20\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/TermGraph/.venv/lib/python3.12/site-packages/openai/_utils/_proxy.py:55\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/TermGraph/.venv/lib/python3.12/site-packages/openai/_module_client.py:12\u001b[0m, in \u001b[0;36mChatProxy.__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchat\n",
      "File \u001b[0;32m~/Programming/TermGraph/.venv/lib/python3.12/site-packages/openai/__init__.py:323\u001b[0m, in \u001b[0;36m_load_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    307\u001b[0m         _client \u001b[38;5;241m=\u001b[39m _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    308\u001b[0m             api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m    309\u001b[0m             azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m             http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[1;32m    320\u001b[0m         )\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[0;32m--> 323\u001b[0m     _client \u001b[38;5;241m=\u001b[39m \u001b[43m_ModuleClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "File \u001b[0;32m~/Programming/TermGraph/.venv/lib/python3.12/site-packages/openai/_client.py:104\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    102\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# print(openai.Engine.list())\n",
    "\n",
    "global_path: Path = Path(\"../../lib/StaticDataStorage/data/Global.json\").resolve()\n",
    "\n",
    "def load_global_static_data() -> Dict[str, Any]:\n",
    "    return json.loads(global_path.read_text())\n",
    "\n",
    "def get_terms(globalJson: Dict[str, Any]) -> List[str]:\n",
    "    ret: List[str] = []\n",
    "\n",
    "    for item in globalJson['terms']:\n",
    "        termDef = item['termDef']\n",
    "        term = termDef.split(\" - \")[0]\n",
    "        ret.append(term)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def get_terms_string() -> str:\n",
    "    globalJson = load_global_static_data()\n",
    "    terms = get_terms(globalJson)\n",
    "    return \", \".join(terms)\n",
    "\n",
    "def print_lines(text: str, max_line_length: int = 120):\n",
    "    lines: List[str] = []\n",
    "    line = \"\"\n",
    "    for word in text.split():\n",
    "        if len(line + word) > max_line_length:\n",
    "            lines.append(line)\n",
    "            line = \"\"\n",
    "        line += word + \" \"\n",
    "    lines.append(line)\n",
    "\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "class ChatEngine:\n",
    "    model = \"\"\n",
    "    temp = 0\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\", temp: float = 0):\n",
    "        self.model = model\n",
    "        self.temp = temp\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class TermChat:\n",
    "    messages: List[Message] = []\n",
    "    engine: ChatEngine\n",
    "\n",
    "    def __init__(self, engine: ChatEngine, system_message: str, user_messages: List[str] = []):\n",
    "        self.engine = engine\n",
    "        self.messages.append(Message(\"system\", system_message))\n",
    "        self.messages.extend([Message(\"user\", message) for message in user_messages])\n",
    "\n",
    "    def get_answer(self, user_message: str = \"\") -> str:\n",
    "        if user_message != \"\":\n",
    "            self.messages.append(Message(\"user\", user_message))\n",
    "\n",
    "        chat_messages = [{\"role\": message.role, \"content\": message.content} for message in self.messages]\n",
    "        response = openai.chat.completions.create(\n",
    "            model=self.engine.model,\n",
    "            messages=chat_messages,\n",
    "            temperature=self.engine.temp,\n",
    "        )\n",
    "        answer = response.choices[0].message[\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        return answer\n",
    "\n",
    "    def clear_messages(self):\n",
    "        first_message = self.messages[0]\n",
    "        self.messages = []\n",
    "        self.messages.append(first_message)\n",
    "\n",
    "\n",
    "TermChat(ChatEngine(), \"\").get_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_term_prompt = \"\"\"You are a terms helper robot \\\n",
    "your task is to guess, based on the list of terms, \\\n",
    "which one term, not in the list, \\\n",
    "is the most relevant to already given terms. Answer with one term please.\"\"\"\n",
    "\n",
    "def predict_next_term():\n",
    "    engine = ChatEngine(temp=0.5)\n",
    "    chat = Chat(engine, continue_term_prompt)\n",
    "    terms_str = get_terms_string()\n",
    "    answer = chat.get_answer(terms_str)\n",
    "    print_lines(answer)\n",
    "    chat.clear_messages()\n",
    "\n",
    "\n",
    "predict_next_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d8465",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_definition_prompt = \"\"\"\n",
    "You are a terms definition robot \\\n",
    "your task is to give a best possible definition to the user requested term. \\\n",
    "\n",
    "You should give a definition step by step, according to the this rules: \\\n",
    "\n",
    "1. Definition should contain only information about what the term intrinsically is, \\\n",
    "without any information about what the term is not, and what the term is related to.\n",
    "Every entity that can be removed from definition without changing the meaning, should be removed.\n",
    "2. Definition should based on the existing list of terms (list would be provided to you).\n",
    "3. Check if there exist terms that also fit this definition and are not synonymous to the term.\n",
    "4. Definition should be as short as possible. \\\n",
    "\n",
    "List of existing terms: '''{0}''' \\\n",
    "\n",
    "User requested definition for term: '''{1}''' \\\n",
    "\n",
    "Format of answer should be step by step thoughts: \\\n",
    "Raw term definition: <try to define user term for the first time. format 'term - definition'> <new line> \\\n",
    "Thoughts about definition: <your thoughts on how this term should be redefined according to rules> <new line> \\\n",
    "\n",
    "First retry: <try to define user term for the second time. format 'term - definition'> <new line> \\\n",
    "Thoughts about definition: <your thoughts on how this term should be redefined according to rules> <new line> \\\n",
    "\n",
    "Final try: <try to define user term for the third time. format 'term - definition'> <new line> \\\n",
    "\"\"\"\n",
    "\n",
    "def give_definition(new_term):\n",
    "    terms_str = get_terms_string()\n",
    "\n",
    "    engine = ChatEngine(temp=0.2)\n",
    "    chat = Chat(engine, give_definition_prompt.format(terms_str, new_term))\n",
    "\n",
    "    answer = chat.get_answer()\n",
    "    print(answer)\n",
    "    chat.clear_messages()\n",
    "\n",
    "give_definition(\"Up quark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939a297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_term(term, definition, area=\"\"):\n",
    "    globalJson = load_global_static_data()\n",
    "\n",
    "    termDef = '{0} - {1}'.format(term, definition)\n",
    "    if area != \"\":\n",
    "        globalJson['terms'].append({\"area\": area, \"termDef\": termDef})\n",
    "    else:\n",
    "        globalJson['terms'].append({\"termDef\": termDef})\n",
    "\n",
    "    with open(global_path, \"w\") as f:\n",
    "        json.dump(globalJson, f, indent=4)\n",
    "\n",
    "add_term(\"Symmetry\", f\"{{system}} {{property}} that remains unchanged after a certain {{transformation}}\", \"phys\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
