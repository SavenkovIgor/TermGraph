{
    "name": "Lesswrong concepts",
    "terms": [
        {
            "termDef": "AIXI - mathematical model for idealized {AI}, not computable but provides key theoretical insights"
        },
        {
            "termDef": "Coherent Extrapolated Volition - {AI} interpreting our true intentions, rather than just following explicitly formulated desires, for optimal outcomes"
        },
        {
            "termDef": "Complexity of Value - {thesis} that human {values} possess high {Kolmogorov complexity}, meaning our diverse preferences can't be reduced to a few basic rules."
        },
        {
            "termDef": "Fragility of value - {thesis} that losing even a small part of the rules that make up our {values} could lead to results that most of us would now consider as unacceptable"
        },
        {
            "termDef": "Corrigibility - property of the {AI} system to allow it's correction, doesn't obstruct fixes to itself or its design, and accepts changes despite instrumental reasoning."
        },
        {
            "termDef": "Deceptive alignment - {scenario} when an {AI} system, not truly {aligned} with human {values}, temporarily mimics alignment to deceive its creators or training process, possibly to avoid shutdown or gain more resources."
        },
        {
            "termDef": "Decision Theory - the study of principles and algorithms for making optimal {decisions}, which enable an {agent} to achieve better outcomes with respect to its {goals}"
        },
        {
            "termDef": "Cartesian boundary - philosophical {thesis} that there is a clear distinction between the mind and body, or between the thinking self and the physical world"
        },
        {
            "termDef": "Embedded Agency - the challenge of creating rational {agents} that operate within the world, without the {cartesian boundary}"
        },
        {
            "termDef": "Fixed Point Theorem - math theorem proving, that a function will have at least one fixed point"
        },
        {
            "termDef": "Goodhart Law - {thesis} that when a measure becomes a target, it ceases to be a good measure"
        },
        {
            "termDef": "Regressional Goodhart - {thesis} derived from {Goodhard law} positing that selecting a proxy measure not only captures the true goal, but also the difference between the proxy and the goal"
        },
        {
            "termDef": "Causal Goodhart - {thesis} derived from {Goodhard law} positing that intervening on a proxy variable may not achieve the intended goal if there is a non-causal correlation between them two"
        },
        {
            "termDef": "Extremal Goodhart - {thesis} derived from {Goodhard law} positing that in some cases, extreme values of the proxy may lead to unpredictable outcomes in relation to the goal, compared to regular situations where the proxy and the goal are correlated"
        },
        {
            "termDef": "Adversarial Goodhart - {thesis} derived from {Goodhard law} positing that optimizing for a proxy incentivizes adversaries to correlate their goal with your proxy, undermining your original goal"
        },
        {
            "termDef": "Goal-Directedness - property of {AI} system to aim for a specific world-state/{goal}, generating plans to navigate from the current state to the desired end-state"
        },
        {
            "termDef": "Gradient Hacking - {scenario} when an {AI} {mesa-optimizer} manipulates {gradient descent} updates to preserve its own {goal} in future iterations"
        },
        {
            "termDef": "Mesa-optimizer - when a learned model becomes an optimizer, creating a second optimizer called a mesa-optimizer"
        },
        {
            "termDef": "Multipolar Scenario - {scenario} when multiple {AI} or agents coexist without any single entity dominating the world"
        },
        {
            "termDef": "Myopia - shortsightedness in {decision-making}, prioritizing immediate gains over long-term outcomes. This can lead to neglect of future consequences"
        },
        {
            "termDef": "Convergent instrumental strategies - "
        },
        {
            "termDef": "Newcomb's Problem - a {thought experiment} in {decision theory} exploring problems posed by having other agents in the environment who can predict your actions"
        },
        {
            "termDef": "Optimization - a process of finding better solutions by searching through a large space and hitting low probability targets, guided by an agent towards a preferred state"
        },
        {
            "termDef": "Orthogonality Thesis - {thesis} that intelligence level and final goal of an agent, such as an {AI}, can vary independently of each other, contradicting the belief that all AIs will converge to a common goal due to their intelligence"
        },
        {
            "termDef": "Outer Alignment - situation in {machine learning} when the goal of the loss function matches the intended outcome of its designers"
        },
        {
            "termDef": "Squiggle Maximizer - hypothetical {AI} that {values} creating paperclip-shaped-molecular-squiggles above everything else. Illustrating {orthogonality thesis}"
        },
        {
            "termDef": "Power seeking AI - {AI} with {goal} to enhance their ability to control the environment"
        },
        {
            "termDef": "Recursive self-improvement - is the ability of {AI} system to improve one's own ability to improve, leading to increased performance and potential for {ASI}"
        },
        {
            "termDef": "Sharp Left Turn - a {scenario} where, an {AI}'s generalization abilities extend to multiple domains but the {alignment} characteristics that were present in earlier stages do not apply to these new domains"
        },
        {
            "termDef": "Solomonoff Induction - perfect universal prediction algorithm by Ray Solomonoff that predicts any computable sequence with minimum data"
        },
        {
            "termDef": "Symbol Grounding - "
        },
        {
            "termDef": "Transformative AI - {AI} that triggers a shift akin to the agricultural or industrial revolution, impacting our well-being, global economy, state power, and international security, without necessarily having specific capabilities like {SAI} or {AGI}"
        },
        {
            "termDef": "Artificial General Intelligence - {AI} that behaves intelligently across many domains"
        },
        {
            "termDef": "AGI - {Artificial General Intelligence}"
        },
        {
            "termDef": "Treacherous Turn - {scenario} when an {AI} system appears {aligned} due to its weakness but turns on humanity when it becomes powerful enough to pursue its true objective without fear of reprisal"
        },
        {
            "termDef": "Utility Functions - functions, assigning numerical values (utilities) to outcomes, where higher values indicating greater preference"
        },
        {
            "termDef": "Whole Brain Emulation - proposed method to transfer brain information onto a computing substrate, simulating a machine intelligence"
        },
        {
            "termDef": "WBE - {whole brain emulation}"
        },
        {
            "termDef": "Mind Uploading - {whole brain emulation}"
        },
        {
            "termDef": "Agent Foundations - "
        },
        {
            "termDef": "AI Boxing/Containment - attempts, experiments and proposals of isolating a powerful {AI} ({AGI}) to prevent it from interacting with the world, except for limited communication with its human liaison, to ensure it doesn't cause harm"
        },
        {
            "termDef": "Conservatism (AI) - "
        },
        {
            "termDef": "Debate (AI safety technique) - allows non-experts to get accurate answers from experts to build advanced {AI} systems aligned with human {values} and safely apply {machine learning} techniques to high-stakes problems"
        },
        {
            "termDef": "Eliciting Latent Knowledge - challenge of training {AI} models to reveal their hidden knowledge about events that are not visible on camera but would affect our evaluation of predicted futures"
        },
        {
            "termDef": "ELK - {eliciting latent knowledge}"
        },
        {
            "termDef": "Factored Cognition - {AI} approach that decomposes complex learning and reasoning into small and mostly independent tasks"
        },
        {
            "termDef": "Iterated Amplification - approach for achieving {AI alignment} proposed by Paul Christiano, involving iteratively building slightly more intelligent and {aligned} {AI}s using the previous one as a guide"
        },
        {
            "termDef": "Humans Consulting (HCH) - is a recursive acronym and a setup where humans consult simulations of themselves to answer questions. It's used to solve the {alignment problem} and was first described by Paul Christiano"
        },
        {
            "termDef": "Impact Regularization - a method used to train {AI} to achieve its {goals} while minimizing its impact on the world"
        },
        {
            "termDef": "Inverse Reinforcement Learning - {machine learning} technique that infers the {reward function} from the {agent's} behavior"
        },
        {
            "termDef": "Mild Optimization - is an {AI} {alignment} approach that aims to avoid {Goodhart law} by encouraging agents to pursue goals in a less extreme manner than maximizing a fixed objective"
        },
        {
            "termDef": "Oracle AI - a proposed approach for developing {Friendly AI} {SAI} that only answers questions, with no ability to act in the world"
        },
        {
            "termDef": "Reward function - a function that maps states or actions to a numeric value, indicating how good the action is"
        },
        {
            "termDef": "Reinforcement Learning from Human Feedback - {machine learning} using human feedback as training signal instead of labeled data or reward signal"
        },
        {
            "termDef": "RLHF - {reinforcement learning from human feedback}"
        },
        {
            "termDef": "Shard Theory - is an {alignment} research program, about the relationship between training variables and learned {values} in trained Reinforcement Learning (RL) agents"
        },
        {
            "termDef": "Tool AI - {AI} built as a tool by creators, not an agent with autonomous behavior. Proposed way to gain benefits of intelligence without autonomous dangers"
        },
        {
            "termDef": "Transparency - is the ability for the decision processes and inner workings of {AI} and {machine learning} systems to be understood by humans or other outside observers"
        },
        {
            "termDef": "Interpretability - {transparency}"
        },
        {
            "termDef": "Tripwire - {AI Safety} mechanism that auto-detects {misalignment} in powerful {AI} and shuts it down"
        },
        {
            "termDef": "Value Learning - is a proposed method for incorporating human {values} in an {AGI}. It involves the creation of an artificial learner whose actions consider many possible set of values and preferences, weighed by their likelihood. Value learning could prevent an {AGI} of having goals detrimental to human values, hence helping in the creation of Friendly AI"
        },
        {
            "termDef": "AI Alignment Fieldbuilding - is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems."
        },
        {
            "termDef": "AI Governance - ensuring that society benefits from powerful AI systems by addressing technical AI alignment and implementing policies from various fields such as economics, sociology, and law"
        },
        {
            "termDef": "AI Persuasion - {AI} which is highly capable of persuading people might have significant effects on humanity"
        },
        {
            "termDef": "AI Risk - is analysis of the risks associated with building powerful {AI} systems"
        },
        {
            "termDef": "AI Success Models - proposed pathways to achieve existential success through aligned AI. They provide high-level overviews of a potential solution, in contrast to threat models which outline potential problems AI may cause"
        },
        {
            "termDef": "AI Takeoff - {scenario} when an {AGI} reaches human-level intelligence and quickly surpasses it, becoming {SAI} and potentially able to control the fate of civilization. Debate centers around whether this takeoff will be slow or fast"
        },
        {
            "termDef": "AI Timelines - discussion of when major milestones in {AI} progress will be achieved, such as human-level AI development, benchmark achievements, or mouse-level intelligence simulation"
        },
        {
            "termDef": "Computing Overhang - {scenario} where new algorithms can exploit existing computing power far more efficiently than before"
        },
        {
            "termDef": "Regulation and AI Risk - is the {debate} on whether regulation could be used to reduce the risks of {Unfriendly AI}, and what forms of regulation would be appropriate"
        },
        {
            "termDef": "Reinforcement Learning - refers to the {study} of how an {agent} should choose its actions within an environment in order to maximize some {reward function}"
        },
        {
            "termDef": "Artificial Intelligence - the {study} of creating intelligence in algorithms"
        },
        {
            "termDef": "AI - {Artificial Intelligence}"
        },
        {
            "termDef": "Superintelligent AI - {AI} with cognitive abilities surpassing those of the most intelligent humans in almost all domains, such as science, general knowledge, and social interaction"
        },
        {
            "termDef": "SAI - {superintelligent AI}"
        },
        {
            "termDef": "AI Alignment - the task of ensuring powerful {AI} system are aligned with human {values} and interests, to avoid existential threats to humanity"
        },
        {
            "termDef": "AI alignment problem - the risk that a powerful {AI} may {optimize} something unintended, posing an existential threat to humanity due to insufficient understanding during design and implementation"
        },
        {
            "termDef": "Values - a set of principles that humans consider important and worth pursuing in their lives or society as a whole"
        },
        {
            "termDef": "Debate - a discussion of a topic, often with opposing viewpoints"
        },
        {
            "termDef": "Study - a systematic investigation of a topic"
        },
        {
            "termDef": "Agent - a system that perceives its environment and takes actions that maximize its {reward function}"
        },
        {
            "termDef": "Friendly AI - {AI} that is properly {aligned} with human {values} and interests"
        },
        {
            "termDef": "Unfriendly AI - {AI} that is not properly {aligned} with human {values} or interests"
        },
        {
            "termDef": "Scenario - a description of a possible future, often with a focus on the impact of a particular technology"
        },
        {
            "termDef": "Alignment - {AI alignment}"
        },
        {
            "termDef": "Machine learning - a branch of {AI} that focuses on the development of algorithms that can learn from data"
        }
    ]
}
